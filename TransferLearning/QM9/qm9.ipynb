{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "dataset = QM9(root='../../Dataset/QM9')\n",
    "# feature 11 维， 用data.y[1] 做标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "training_set, test_set  = random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)], \\\n",
    "    generator=torch.Generator().manual_seed(42))\n",
    "training_set, validation_set = random_split(training_set, [int(len(training_set) * 0.9), len(training_set) - int(len(training_set) * 0.9)], \\\n",
    "    generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94197, 10467, 26167)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set), len(validation_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "def seed_torch(seed=42):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed) # if you are using multi-GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, precision_score, f1_score, recall_score\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from torch.utils.data import random_split\n",
    "import pickle\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "        num_classses = 1\n",
    "\n",
    "        conv_hidden = args['conv_hidden']\n",
    "        cls_hidden = args['cls_hidden']\n",
    "        self.n_layers = args['n_layers']\n",
    "\n",
    "        self.conv_layers = nn.ModuleList([])\n",
    "\n",
    "        self.conv1 = SAGEConv(11, conv_hidden)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            self.conv_layers.append(\n",
    "                SAGEConv(conv_hidden, conv_hidden)\n",
    "            )\n",
    "\n",
    "        self.linear1 = nn.Linear(conv_hidden, cls_hidden)\n",
    "        self.linear2 = nn.Linear(cls_hidden, num_classses)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, mol):\n",
    "\n",
    "        res = self.conv1(mol.x, mol.edge_index)\n",
    "        res = self.relu(res)\n",
    "        for i in range(self.n_layers):\n",
    "            res = self.relu(self.conv_layers[i](res, mol.edge_index))\n",
    "\n",
    "        res = global_mean_pool(res, mol.batch)\n",
    "        res = self.linear1(res)\n",
    "        res = self.relu(res)\n",
    "        res = self.drop1(res)\n",
    "        res = self.linear2(res)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, training_set, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for mol in training_set:\n",
    "        mol = mol.to(device)\n",
    "        mol.x = mol.x\n",
    "        target = mol.y[:, 1]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mol)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Train Epoch: {epoch}, Ave Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(args, model, device, val_set, optimizer, criterion, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for mol in val_set:\n",
    "        mol = mol.to(device)\n",
    "        mol.x = mol.x\n",
    "        target = mol.y[:, 1]\n",
    "        optimizer.zero_grad()\n",
    "        output= model(mol)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'val Epoch: {epoch}, Ave Loss: {total_loss}')\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_set, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for mol in test_set:\n",
    "            mol = mol.to(device)\n",
    "            mol.x = mol.x\n",
    "            target = mol.y[:, 1]\n",
    "            output = model(mol)\n",
    "            loss = criterion(output, target)            \n",
    "    print(f'test loss: {loss}')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed_torch(args['seed'])\n",
    "    model = Model(args).to(device)\n",
    "    print(model)\n",
    "    batch_size = args['batch_size']\n",
    "    train_loader = DataLoader(training_set, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(validation_set, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    min_loss = 10000\n",
    "    for epoch in range(1, args['epoch'] + 1):\n",
    "        train(args, model, device, train_loader, optimizer, loss_fn, epoch)\n",
    "        val_loss = val(args, model, device, val_loader, optimizer, loss_fn, epoch)\n",
    "        scheduler.step()\n",
    "        # nni.report_intermediate_result(val_auc)\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            print('Saving model (epoch = {:4d}, top2acc = {:.4f})'\n",
    "                .format(epoch, val_loss))\n",
    "            torch.save(model.state_dict(), args['save_path'])\n",
    "    # final result\n",
    "    model.load_state_dict(torch.load(args['save_path']))\n",
    "    final_loss = test(model, device, test_loader, loss_fn)\n",
    "    # nni.report_final_result(final_auc)\n",
    "    print(final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'lr': 0.0001, # 0.001 就会nan\n",
    "    'epoch': 100,\n",
    "    'seed': 42,\n",
    "    'save_path': './model/model',\n",
    "    'conv_hidden':1024,\n",
    "    'cls_hidden':512,\n",
    "    'n_layers':3,\n",
    "    'batch_size':128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): SAGEConv(1024, 1024)\n",
      "    (1): SAGEConv(1024, 1024)\n",
      "    (2): SAGEConv(1024, 1024)\n",
      "  )\n",
      "  (conv1): SAGEConv(11, 1024)\n",
      "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Train Epoch: 1, Ave Loss: 1659271.231956482\n",
      "val Epoch: 1, Ave Loss: 15952.446632385254\n",
      "Train Epoch: 2, Ave Loss: 118089.00852966309\n",
      "val Epoch: 2, Ave Loss: 6638.082328796387\n",
      "Saving model (epoch =    2, top2acc = 6638.0823)\n",
      "Train Epoch: 3, Ave Loss: 98393.75959777832\n",
      "val Epoch: 3, Ave Loss: 5945.192359924316\n",
      "Saving model (epoch =    3, top2acc = 5945.1924)\n",
      "Train Epoch: 4, Ave Loss: 93602.94033050537\n",
      "val Epoch: 4, Ave Loss: 5944.856391906738\n",
      "Saving model (epoch =    4, top2acc = 5944.8564)\n",
      "Train Epoch: 5, Ave Loss: 90316.35327148438\n",
      "val Epoch: 5, Ave Loss: 5833.482570648193\n",
      "Saving model (epoch =    5, top2acc = 5833.4826)\n",
      "Train Epoch: 6, Ave Loss: 89854.37446594238\n",
      "val Epoch: 6, Ave Loss: 5744.412223815918\n",
      "Saving model (epoch =    6, top2acc = 5744.4122)\n",
      "Train Epoch: 7, Ave Loss: 89061.65717315674\n",
      "val Epoch: 7, Ave Loss: 5745.022926330566\n",
      "Train Epoch: 8, Ave Loss: 88536.37393188477\n",
      "val Epoch: 8, Ave Loss: 5671.911422729492\n",
      "Saving model (epoch =    8, top2acc = 5671.9114)\n",
      "Train Epoch: 9, Ave Loss: 88275.8723526001\n",
      "val Epoch: 9, Ave Loss: 5656.925758361816\n",
      "Saving model (epoch =    9, top2acc = 5656.9258)\n",
      "Train Epoch: 10, Ave Loss: 87881.80715179443\n",
      "val Epoch: 10, Ave Loss: 5654.892547607422\n",
      "Saving model (epoch =   10, top2acc = 5654.8925)\n",
      "Train Epoch: 11, Ave Loss: 87744.32596588135\n",
      "val Epoch: 11, Ave Loss: 5635.719123840332\n",
      "Saving model (epoch =   11, top2acc = 5635.7191)\n",
      "Train Epoch: 12, Ave Loss: 87696.19004058838\n",
      "val Epoch: 12, Ave Loss: 5641.030082702637\n",
      "Train Epoch: 13, Ave Loss: 87407.80139923096\n",
      "val Epoch: 13, Ave Loss: 5635.721111297607\n",
      "Train Epoch: 14, Ave Loss: 87403.62547302246\n",
      "val Epoch: 14, Ave Loss: 5633.628330230713\n",
      "Saving model (epoch =   14, top2acc = 5633.6283)\n",
      "Train Epoch: 15, Ave Loss: 87040.29692077637\n",
      "val Epoch: 15, Ave Loss: 5624.5472984313965\n",
      "Saving model (epoch =   15, top2acc = 5624.5473)\n",
      "Train Epoch: 16, Ave Loss: 87137.48532104492\n",
      "val Epoch: 16, Ave Loss: 5624.989418029785\n",
      "Train Epoch: 17, Ave Loss: 87150.17945861816\n",
      "val Epoch: 17, Ave Loss: 5617.679069519043\n",
      "Saving model (epoch =   17, top2acc = 5617.6791)\n",
      "Train Epoch: 18, Ave Loss: 86681.7332611084\n",
      "val Epoch: 18, Ave Loss: 5611.525787353516\n",
      "Saving model (epoch =   18, top2acc = 5611.5258)\n",
      "Train Epoch: 19, Ave Loss: 86614.83261108398\n",
      "val Epoch: 19, Ave Loss: 5612.983943939209\n",
      "Train Epoch: 20, Ave Loss: 86481.11652374268\n",
      "val Epoch: 20, Ave Loss: 5607.0741539001465\n",
      "Saving model (epoch =   20, top2acc = 5607.0742)\n",
      "Train Epoch: 21, Ave Loss: 86590.3401260376\n",
      "val Epoch: 21, Ave Loss: 5600.175384521484\n",
      "Saving model (epoch =   21, top2acc = 5600.1754)\n",
      "Train Epoch: 22, Ave Loss: 86347.26512908936\n",
      "val Epoch: 22, Ave Loss: 5611.176296234131\n",
      "Train Epoch: 23, Ave Loss: 86343.84043884277\n",
      "val Epoch: 23, Ave Loss: 5605.723304748535\n",
      "Train Epoch: 24, Ave Loss: 86518.51187896729\n",
      "val Epoch: 24, Ave Loss: 5598.713115692139\n",
      "Saving model (epoch =   24, top2acc = 5598.7131)\n",
      "Train Epoch: 25, Ave Loss: 86538.36547088623\n",
      "val Epoch: 25, Ave Loss: 5601.612564086914\n",
      "Train Epoch: 26, Ave Loss: 86529.46469116211\n",
      "val Epoch: 26, Ave Loss: 5605.143009185791\n",
      "Train Epoch: 27, Ave Loss: 86501.34311676025\n",
      "val Epoch: 27, Ave Loss: 5598.480400085449\n",
      "Saving model (epoch =   27, top2acc = 5598.4804)\n",
      "Train Epoch: 28, Ave Loss: 86215.4670715332\n",
      "val Epoch: 28, Ave Loss: 5599.24772644043\n",
      "Train Epoch: 29, Ave Loss: 86375.4503250122\n",
      "val Epoch: 29, Ave Loss: 5597.354145050049\n",
      "Saving model (epoch =   29, top2acc = 5597.3541)\n",
      "Train Epoch: 30, Ave Loss: 85990.08209991455\n",
      "val Epoch: 30, Ave Loss: 5595.8670654296875\n",
      "Saving model (epoch =   30, top2acc = 5595.8671)\n",
      "Train Epoch: 31, Ave Loss: 86006.4408493042\n",
      "val Epoch: 31, Ave Loss: 5596.043663024902\n",
      "Train Epoch: 32, Ave Loss: 85872.33937072754\n",
      "val Epoch: 32, Ave Loss: 5591.703353881836\n",
      "Saving model (epoch =   32, top2acc = 5591.7034)\n",
      "Train Epoch: 33, Ave Loss: 86147.57724761963\n",
      "val Epoch: 33, Ave Loss: 5594.875633239746\n",
      "Train Epoch: 34, Ave Loss: 85947.01236724854\n",
      "val Epoch: 34, Ave Loss: 5589.468193054199\n",
      "Saving model (epoch =   34, top2acc = 5589.4682)\n",
      "Train Epoch: 35, Ave Loss: 86229.9534072876\n",
      "val Epoch: 35, Ave Loss: 5601.319206237793\n",
      "Train Epoch: 36, Ave Loss: 86229.59103393555\n",
      "val Epoch: 36, Ave Loss: 5595.864719390869\n",
      "Train Epoch: 37, Ave Loss: 86102.88791656494\n",
      "val Epoch: 37, Ave Loss: 5588.587985992432\n",
      "Saving model (epoch =   37, top2acc = 5588.5880)\n",
      "Train Epoch: 38, Ave Loss: 86157.46075439453\n",
      "val Epoch: 38, Ave Loss: 5594.190086364746\n",
      "Train Epoch: 39, Ave Loss: 85913.73344421387\n",
      "val Epoch: 39, Ave Loss: 5597.498634338379\n",
      "Train Epoch: 40, Ave Loss: 85981.85263061523\n",
      "val Epoch: 40, Ave Loss: 5597.375202178955\n",
      "Train Epoch: 41, Ave Loss: 85996.79243469238\n",
      "val Epoch: 41, Ave Loss: 5590.716728210449\n",
      "Train Epoch: 42, Ave Loss: 85963.39960479736\n",
      "val Epoch: 42, Ave Loss: 5587.464786529541\n",
      "Saving model (epoch =   42, top2acc = 5587.4648)\n",
      "Train Epoch: 43, Ave Loss: 85709.4476852417\n",
      "val Epoch: 43, Ave Loss: 5590.248790740967\n",
      "Train Epoch: 44, Ave Loss: 85672.11917114258\n",
      "val Epoch: 44, Ave Loss: 5590.328109741211\n",
      "Train Epoch: 45, Ave Loss: 85498.17995452881\n",
      "val Epoch: 45, Ave Loss: 5593.034122467041\n",
      "Train Epoch: 46, Ave Loss: 85421.7942199707\n",
      "val Epoch: 46, Ave Loss: 5588.499942779541\n",
      "Train Epoch: 47, Ave Loss: 85853.38437652588\n",
      "val Epoch: 47, Ave Loss: 5584.941257476807\n",
      "Saving model (epoch =   47, top2acc = 5584.9413)\n",
      "Train Epoch: 48, Ave Loss: 85400.16487884521\n",
      "val Epoch: 48, Ave Loss: 5592.7366065979\n",
      "Train Epoch: 49, Ave Loss: 85427.87218475342\n",
      "val Epoch: 49, Ave Loss: 5586.381732940674\n",
      "Train Epoch: 50, Ave Loss: 85244.56453704834\n",
      "val Epoch: 50, Ave Loss: 5592.712448120117\n",
      "Train Epoch: 51, Ave Loss: 85771.71924591064\n",
      "val Epoch: 51, Ave Loss: 5588.946720123291\n",
      "Train Epoch: 52, Ave Loss: 85574.95861053467\n",
      "val Epoch: 52, Ave Loss: 5585.055881500244\n",
      "Train Epoch: 53, Ave Loss: 85424.17015075684\n",
      "val Epoch: 53, Ave Loss: 5591.294078826904\n",
      "Train Epoch: 54, Ave Loss: 85131.22367095947\n",
      "val Epoch: 54, Ave Loss: 5587.073020935059\n",
      "Train Epoch: 55, Ave Loss: 85656.39095306396\n",
      "val Epoch: 55, Ave Loss: 5586.95276260376\n",
      "Train Epoch: 56, Ave Loss: 85303.32485198975\n",
      "val Epoch: 56, Ave Loss: 5590.699317932129\n",
      "Train Epoch: 57, Ave Loss: 85149.88428497314\n",
      "val Epoch: 57, Ave Loss: 5587.383232116699\n",
      "Train Epoch: 58, Ave Loss: 85265.44368743896\n",
      "val Epoch: 58, Ave Loss: 5584.178829193115\n",
      "Saving model (epoch =   58, top2acc = 5584.1788)\n",
      "Train Epoch: 59, Ave Loss: 85432.8233795166\n",
      "val Epoch: 59, Ave Loss: 5584.579837799072\n",
      "Train Epoch: 60, Ave Loss: 85250.45825958252\n",
      "val Epoch: 60, Ave Loss: 5588.929954528809\n",
      "Train Epoch: 61, Ave Loss: 85403.11868286133\n",
      "val Epoch: 61, Ave Loss: 5586.588829040527\n",
      "Train Epoch: 62, Ave Loss: 85211.48162841797\n",
      "val Epoch: 62, Ave Loss: 5588.118476867676\n",
      "Train Epoch: 63, Ave Loss: 85196.0086517334\n",
      "val Epoch: 63, Ave Loss: 5582.113586425781\n",
      "Saving model (epoch =   63, top2acc = 5582.1136)\n",
      "Train Epoch: 64, Ave Loss: 85265.5950088501\n",
      "val Epoch: 64, Ave Loss: 5588.387020111084\n",
      "Train Epoch: 65, Ave Loss: 85206.14488220215\n",
      "val Epoch: 65, Ave Loss: 5585.099960327148\n",
      "Train Epoch: 66, Ave Loss: 85202.24948883057\n",
      "val Epoch: 66, Ave Loss: 5594.700714111328\n",
      "Train Epoch: 67, Ave Loss: 85170.07942199707\n",
      "val Epoch: 67, Ave Loss: 5589.384910583496\n",
      "Train Epoch: 68, Ave Loss: 85583.89102935791\n",
      "val Epoch: 68, Ave Loss: 5586.935688018799\n",
      "Train Epoch: 69, Ave Loss: 85333.20738983154\n",
      "val Epoch: 69, Ave Loss: 5586.300399780273\n",
      "Train Epoch: 70, Ave Loss: 85165.98112487793\n",
      "val Epoch: 70, Ave Loss: 5585.16849899292\n",
      "Train Epoch: 71, Ave Loss: 85078.85210418701\n",
      "val Epoch: 71, Ave Loss: 5583.646240234375\n",
      "Train Epoch: 72, Ave Loss: 85003.08656311035\n",
      "val Epoch: 72, Ave Loss: 5589.226219177246\n",
      "Train Epoch: 73, Ave Loss: 85239.30364990234\n",
      "val Epoch: 73, Ave Loss: 5579.107948303223\n",
      "Saving model (epoch =   73, top2acc = 5579.1079)\n",
      "Train Epoch: 74, Ave Loss: 85178.25128936768\n",
      "val Epoch: 74, Ave Loss: 5589.5419921875\n",
      "Train Epoch: 75, Ave Loss: 84832.94943237305\n",
      "val Epoch: 75, Ave Loss: 5586.216728210449\n",
      "Train Epoch: 76, Ave Loss: 85134.1753692627\n",
      "val Epoch: 76, Ave Loss: 5593.2639236450195\n",
      "Train Epoch: 77, Ave Loss: 85061.53213500977\n",
      "val Epoch: 77, Ave Loss: 5583.157173156738\n",
      "Train Epoch: 78, Ave Loss: 85022.54969787598\n",
      "val Epoch: 78, Ave Loss: 5594.304389953613\n",
      "Train Epoch: 79, Ave Loss: 85462.70347595215\n",
      "val Epoch: 79, Ave Loss: 5587.415096282959\n",
      "Train Epoch: 80, Ave Loss: 84859.98944091797\n",
      "val Epoch: 80, Ave Loss: 5581.256923675537\n",
      "Train Epoch: 81, Ave Loss: 85135.89530181885\n",
      "val Epoch: 81, Ave Loss: 5585.555492401123\n",
      "Train Epoch: 82, Ave Loss: 85189.87772369385\n",
      "val Epoch: 82, Ave Loss: 5590.519992828369\n",
      "Train Epoch: 83, Ave Loss: 84963.76359558105\n",
      "val Epoch: 83, Ave Loss: 5583.795211791992\n",
      "Train Epoch: 84, Ave Loss: 85113.59912872314\n",
      "val Epoch: 84, Ave Loss: 5585.381591796875\n",
      "Train Epoch: 85, Ave Loss: 85131.59615325928\n",
      "val Epoch: 85, Ave Loss: 5585.725131988525\n",
      "Train Epoch: 86, Ave Loss: 84838.38230133057\n",
      "val Epoch: 86, Ave Loss: 5585.570026397705\n",
      "Train Epoch: 87, Ave Loss: 85150.34764099121\n",
      "val Epoch: 87, Ave Loss: 5588.915859222412\n",
      "Train Epoch: 88, Ave Loss: 85181.3643951416\n",
      "val Epoch: 88, Ave Loss: 5589.827255249023\n",
      "Train Epoch: 89, Ave Loss: 85177.74584960938\n",
      "val Epoch: 89, Ave Loss: 5586.658889770508\n",
      "Train Epoch: 90, Ave Loss: 85019.71132659912\n",
      "val Epoch: 90, Ave Loss: 5585.875690460205\n",
      "Train Epoch: 91, Ave Loss: 84958.69404602051\n",
      "val Epoch: 91, Ave Loss: 5587.652210235596\n",
      "Train Epoch: 92, Ave Loss: 84860.53914642334\n",
      "val Epoch: 92, Ave Loss: 5586.441249847412\n",
      "Train Epoch: 93, Ave Loss: 84925.7763748169\n",
      "val Epoch: 93, Ave Loss: 5589.968486785889\n",
      "Train Epoch: 94, Ave Loss: 85438.42419433594\n",
      "val Epoch: 94, Ave Loss: 5590.239368438721\n",
      "Train Epoch: 95, Ave Loss: 84925.1466293335\n",
      "val Epoch: 95, Ave Loss: 5591.852085113525\n",
      "Train Epoch: 96, Ave Loss: 85177.42060852051\n",
      "val Epoch: 96, Ave Loss: 5592.972019195557\n",
      "Train Epoch: 97, Ave Loss: 85134.0375213623\n",
      "val Epoch: 97, Ave Loss: 5587.636734008789\n",
      "Train Epoch: 98, Ave Loss: 84981.12135314941\n",
      "val Epoch: 98, Ave Loss: 5585.6062660217285\n",
      "Train Epoch: 99, Ave Loss: 84941.34508514404\n",
      "val Epoch: 99, Ave Loss: 5601.350326538086\n",
      "Train Epoch: 100, Ave Loss: 85015.13207244873\n",
      "val Epoch: 100, Ave Loss: 5585.600448608398\n",
      "test loss: 77.18523406982422\n",
      "tensor(77.1852, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgl/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([55])) that is different to the input size (torch.Size([55, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae2bdcb5ffd42edc58b1d6fb8428ae1d2700e79a4fc0c0139ea5d98047639f54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
